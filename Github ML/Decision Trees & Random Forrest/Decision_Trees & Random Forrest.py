# -*- coding: utf-8 -*-
"""Decision Trees And Random Forrest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6uRgd2dqb8Kq5Fa7A39EaxX_hC1t8w0
"""

!pip install -Uqq fastbook
!pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz
import fastbook
fastbook.setup_book()
from fastbook import *
from fastai.vision.widgets import *
from google.colab import drive
from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype
from fastai.tabular.all import *
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from dtreeviz.trees import *
from IPython.display import Image, display_svg, SVG
drive.mount('/content/drive')

!pip install kaggle

!mkdir ~/.kaggle
!echo '{"username":"","key":""}' > ~/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json


from kaggle import api

path = URLs.path('bluebook')
path
Path.BASE_PATH = path

if not path.exists():
    path.mkdir(parents=true)
    api.competition_download_cli('bluebook-for-bulldozers', path=path)
    file_extract(path/'bluebook-for-bulldozers.zip')
path.ls(file_type='text')

df = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df = add_datepart(df, 'saledate') #adds data such as year of sale , month of sale etc relating to dates

df_test = pd.read_csv(path/'Test.csv', low_memory=False)
df_test = add_datepart(df_test, 'saledate')#adds data such as year of sale , month of sale etc relating to dates

x = [o for o in df.columns if o.startswith('sale')]
# x #Shows all the new columns that were added , information that was extracted via add_datepart

procs = [FillMissing,Categorify]#Categorify is a TabularProc that replaces 
#a column with a numeric categorical column. FillMissing is a TabularProc 
#that replaces missing values with the median of the column, 
#and creates a new Boolean column that is set to True for any row where the value was missing.

cond = ((df.saleYear<=2010) | ((df.saleYear==2011) & (df.saleMonth<=10)))

train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]
#we will let validation set be dates after  November 2011
splits = (list(train_idx),list(valid_idx))

pd.options.display.max_rows = 20
pd.options.display.max_columns = 10

df[["saleYear","saleMonth"]]

dep_var = 'SalePrice'#dependant variable
df[dep_var] = np.log(df[dep_var])


cont,cat = cont_cat_split(df, 1, dep_var=dep_var)#split our columns into  continuous data columns (salesID,yearmade etc) and categorical

tt = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)

len(tt.cont_names) #names of the continous variables
len(tt.cat_names) == len(tt.classes) #classes only include categorical data

save_pickle(path/'tt.pkl',tt)
tt = load_pickle(path/'tt.pkl')

xs,y = tt.train.xs,tt.train.y
valid_xs,valid_y = tt.valid.xs,tt.valid.y

m = DecisionTreeRegressor(max_leaf_nodes=4)#draw 
m.fit(xs, y);

draw_tree(m, xs, size=11, leaves_parallel=True, precision=2)

# What we have below is a decision tree. It starts off at the top node and makes a binary split ,
# whether the value for the Coupler_System is less than 0.5 or not and keeps making splits for 3 more levels.



samp_idx = np.random.permutation(len(y))[:500]
dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,
        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,
        orientation='LR')

#As seen above , in the YearMade Split we can see that ther are bulldozers made in 1000 ... that's just wrong, 
#this may be due to missing values so lets replace any date before 1900 with an aribtrary year , 1950 
xs.loc[xs['YearMade']<1900, 'YearMade'] = 1950
valid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950
# df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
#                    columns=['a', 'b', 'c'])


# #for renaming values , lets say change all values in column "a" which are less than 3 to"less than 3"

# df2.loc[df2['a']<3 , 'a'] = "less than 3"
# df2

samp_idx = np.random.permutation(len(y))[:500]
dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,
        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,
        orientation='LR')

"""The image above allows us to look into our data better

"""

m = DecisionTreeRegressor()
m.fit(xs, y);#let's make a decision tree  with no limit on the max number of leaf nodes

def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)
def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) #predict on data (xs) and compare with label (y)

m_rmse(m, xs, y)#Amazing , our RMSE is essentially 0.

m_rmse(m, valid_xs, valid_y) # However when we try this on our Validation set, our RMSE is pretty large ...

m.get_n_leaves(), len(xs)#Well this is because, our Decision Tree has almost the same number of leaves, end nodes as it does
#data points ... we have overfitted our Decision Tree

#The problem above can be solved if we just set the minimum number of datapoints that needs to be in each leaf , this prevents our
#model from fitting too closesly to our training dataset.
m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(xs, y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)

m.get_n_leaves(), len(xs)

#A Random Forest is merely an ensemble of Decision Trees. The difference here is that instead of using the whole training dataset ,
#in the random forest approach each tree takes a subset of the training dataset and creates a decision tree. As such each decision tree
#gets a good understanding of the subset of the dataset it has chosen. However each tree alone will perform rather poorly 
#if it is used as an end product. As such by combining all the trees that studied different subsets of the datasets 
#we get a model that has a good understanding of our entire training dataset. We can liken this to the following scenario :

#You want to go to a particular Restaurant however you aren't sure if you should. Naturally , you ask you friend Joseph
#he says yes because the restraunt is Affordable and has good Lobster. You check online and the reviews
#say that the customer service is poor and their Salad is bad. With both these pieces of information, you then make an informed 
#decision of whether or not to go to the Restaurant. Just like this , a Random Forest Approach uses the knowledge provided by multiple 
#decision trees to give us one final output of a better accuracy than the traditional Decision Tree approach.(if executed correctly)
#Now ,let's create our Random Forest 

def rf(xs, y, n_estimators=40, max_samples=200_000,
       max_features=0.5, min_samples_leaf=5, **kwargs):#xs and y are our dependent and dependent variables respectively , 
       #n_estimators tells us to generate 40 trees 
       #max_samples tells us how many randomly chosen rows(horizontal) to sample for each tree if we have more than 200,000 datapoints we set it to equal to 200,000
       #if not we let it revert to default value
      #min_samples_leaf sets the minimum number of samples(data points)  to be at each node
    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
        max_samples=max_samples, max_features=max_features,
        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)

random_forest_classifier = rf(xs, y);

m_rmse(random_forest_classifier, xs, y), m_rmse(random_forest_classifier, valid_xs, valid_y)#drastic improvement from using the standard 
#decision tree

preds = np.stack([t.predict(valid_xs) for t in random_forest_classifier.estimators_])

pd.DataFrame(data=preds )
#the 0 axis (horizontals) shows the predicitions given by each of the 40 random_forest trees we generated , for each datapoint in
#valid_xs(prediction made by each random_forest tree on valid_xs)

# a = np.array([[1, 2], [3, 4]])
# a.mean(0)

preds.mean(0) #lets get the mean of  each column (vertical  )
#this gives us the average price predicition (across all our 40 trees) for each of the 17327 auctions in out validation set

r_mse(preds.mean(0), valid_y)
#SEE , SAME AS m_rmse(random_forest_classifier, valid_xs, valid_y)

plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);
#Treat for the eyes let's see how rmse improves with more trees being added

pd.DataFrame(data=preds )
#the rows (horzontals) show the predicitions given by each of the 40 random_forest trees we generated , for each datapoint in
#valid_xs(prediction made by each random_forest tree on valid_xs)

preds_std = preds.std(0)#lets get the std of each predicitons made on the same datapoint by each our 40 treees

# a = np.array([[1, 2], [3, 4]])
# a.mean(0)# a = np.array([[1, 2], [3, 4]])
# a.mean(0)

preds_std[:5]#As seen, variance fluctuates quite abit this is because on some auctions the trees agree but for the one with high
#std the trees don't quite agree. As such we could say that the confidence in the predcitions is rather varied

r_mse(random_forest_classifier.oob_prediction_, y)#Random Forest uses the bagging approach, thus there are subsets of our training dataset
#that would not have been used for training the tree as such we can now validate our trees against the subsets that 
#it wasnt trained on. The error we attain from this is called Out Of Bag Error

#note that comparisons are made to the training labels (y) instead of valid_y since the subsets we are prediciting on come
#from the training dataset

def rf_feat_importance(random_forest_classifier, df):
    return pd.DataFrame({'cols':df.columns, 'imp':random_forest_classifier.feature_importances_}
                       ).sort_values('imp', ascending=False)

#A function that can give features of our dataset that played the most part in affecting the price of the bulldozer

fi = rf_feat_importance(random_forest_classifier, xs)
fi[:10]

#What's going on ?

#We start at the top of our first treee and trickle down, at each split we see which column was used to split the data 
#we compare the model's prediction before and after the split and relate the Increase or Decrease in the price to the column that
#was used as the binary splitter. We do this to all our 40 trees and add  the Improvement/Disimprovement for all the columns in all the trees , normalize them 
#so they sum up to one and we get the feature importance as shown below.

fi.sum(0)["imp"] #sum of all the values in the horizontals of the column "imp"
#Proven to be essentially 1

def plot_fi(fi):
    return fi.plot('cols', 'imp', 'barh', figsize=(15,10), legend=False)

plot_fi(fi[:30]);

#Could we imporve our Random Forest Classifier by removing the variables of low importance ?

to_keep = fi[fi["imp"]>0.005].cols#only keep columns with importance of greater than 0.005

to_keep

xs_imp = xs[to_keep]
valid_xs_imp = valid_xs[to_keep]

dropped_random_forest_classifier = rf(xs_imp, y)

m_rmse(dropped_random_forest_classifier, xs_imp, y), m_rmse(dropped_random_forest_classifier, valid_xs_imp, valid_y)

#Accuracy seems to have gotten worse ...

len(xs.columns), len(xs_imp.columns)

plot_fi(rf_feat_importance(dropped_random_forest_classifier, xs_imp));

#Some categories have similar meanings and hence are just adding to the cloud around our data ...
#ProductGroup and ProductGroupDesc more or less tell us the same thing right ... 
#Let's Find out

cluster_columns(xs_imp)

#As Shown in the plot above, we can see that some columns basically refer to the same thing such as ProductGroup and ProductGroupDesc 
#Logically we can remove those Columns too

#OOB score is a number returned by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model
#Now lets create a function to create a Random Forest and and return us the OOB Score we are using a function as we 
#don't necessarily want to create and keep the Random Forest but rather see how the removal of certain groups (groups we deem to
# be referring to the same thing ) affect our model.

def get_oob(xs_imp ,y):
    RandomForestInFunction = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,
        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)
    RandomForestInFunction.fit(xs_imp, y)

    oob_score  = RandomForestInFunction.oob_score_
    # print("oob_score_ : {oob_score}".format(oob_score=RandomForestInFunction.oob_score_) )
    return RandomForestInFunction.oob_score_

get_oob(xs_imp,y)

{"Removal of Column {c}".format(c=c):get_oob(xs_imp.drop(c, axis=1) , y) for c in (
    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',
    'fiModelDesc', 'fiBaseModel',
    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}

#Let's try dropping multiple columns , specifically one of each of the columns we deemed to be very closely related to each other,
#Based on the cluster columns
to_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']
get_oob(xs_imp.drop(to_drop, axis=1) , y)

#Now , let's try dropping multiple columns , the ones that gave us the best oob_score_ when removed from the dataframe

to_drop = ['Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'ProductGroup']
get_oob(xs_imp.drop(to_drop, axis=1),y)

#since the columns that were dropped above gave us a better oob_score_ we drop those and test against our validation set
to_drop = ['Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'ProductGroup','saleYear']
xs_final = xs_imp.drop(to_drop, axis=1)
valid_xs_final = valid_xs_imp.drop(to_drop, axis=1)
random_forest_classifier_with_columns_dropped = rf(xs_final, y)
m_rmse(random_forest_classifier_with_columns_dropped, xs_final, y), m_rmse(random_forest_classifier_with_columns_dropped, valid_xs_final, valid_y)

#We could also find if there is a dependency between certain categories and sale price of the bulldozer


p = valid_xs_final['ProductSize'].value_counts(sort=True).plot.barh()
c = tt.classes['ProductSize']
plt.yticks(range(7), c);
#na is the label fastAi gives if we didnt specify the value for size

ax = valid_xs_final['YearMade'].hist()

from sklearn.inspection import plot_partial_dependence

fig,ax = plt.subplots(figsize=(12, 4))
plot_partial_dependence(random_forest_classifier_with_columns_dropped, valid_xs_final, ['YearMade','ProductSize'],
                        grid_resolution=20, ax=ax);

#What's going on Above ?
#Aren't we just taking the average price of all the predicted sales made in the given year and plotting it ? 
#Well ... no, that tells us how all the fields(columns) affect the predicted price. However what we want to find is the 
#effect of year on the selling price of the bulldozer at auction.

#To do this , we replace the value for year with the earliest year we have in the  "YearMade" column (we do this to all the vlaues) 
# then we use our Random Forest predictor to predict the price of the bulldozer auction. With this we can see how the change in YearMade 
#affects the predicted auction price ... hence the partial dependence of the Auction price and the year that the bulldozer was made

import warnings
warnings.simplefilter('ignore', FutureWarning)

from treeinterpreter import treeinterpreter
from waterfall_chart import plot as waterfall

#We have already deciphered how feature importance plays a part when we look at predicting across the whole dataset but we can do this
#for certain rows too (horiznotals i.e for a handfull of different tractors with different attributes )

#We use a handfull of columns and start at the top of our first treee and trickle down, at each split we see which column was used to split the data 
#we compare the model's prediction before and after the split and relate the Increase or Decrease in the price to the column that
#was used as the binary splitter. We do this to all our 40 trees and add  the Improvement/Disimprovement for all the columns in all the trees , normalize them 
#so they sum up to one and we get the treeinterpreter as shown below.

row = valid_xs_final.iloc[:30]

prediction,bias,contributions = treeinterpreter.predict(random_forest_classifier_with_columns_dropped, row.values)

prediction[0], bias[0], contributions[0].sum() #for the first row
#Bias is the first prediction made by the 1st node of the Random Forest. Contributuons are the changes in each predicition as we 
#head down the Random Forest Tree the sum of the Contributions added to the Bias will give us Predicition. 
#Where Prediciton is the price of the auctioned off Bulldozer.

waterfall(valid_xs_final.columns, contributions[0], threshold=0.2, #threshold of 
          rotation_value=45,formatting='{:,.3f}' );

#We can use this after production to show users why a certain bulldozer was predicted to be auctioned off at a certain price 
#given them insights as to why the Random Forest predicted the price as it di.

#The issue with Random Forest Regressors as with most ML algorithms is the fact that they dont perform too well with new data

np.random.seed(42)

x_lin = torch.linspace(0,20, steps=40)
y_lin = x_lin + torch.randn_like(x_lin)
plt.scatter(x_lin, y_lin);

xs_lin = x_lin.unsqueeze(1) #we need to give our random forect a matrix independent variables, not a single vector
x_lin.shape,xs_lin.shape

m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30]) #use the first 30 data points as our training set

plt.scatter(x_lin, y_lin, 20)
plt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5)

#As Shown above , our random forest classifier does amazing in prediciting data it has come across or data similar to what it has 
# seen before but when it sees brand new data , it is completely off

#As such when using Random Forest Regressors we need to make sure that our training and validation sets are not worlds apart,
#... How do we do that ?

#Since Random Forest can do classification too , we could just merge both our training and validation sets , then 
#create an array that acts as a binary label if a particular row is part of the training or validation set. 
#We could then use our Random Forest to predict , based on the details in a row whether it is part of the training or validation set.
#Then we use feature importance to understand what columns the model uses to identify if the 
#row is part of the training or validation set

df_dom = pd.concat([xs_final, valid_xs_final])
is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))

m = rf(df_dom, is_valid)
rf_feat_importance(m, df_dom)[:6]

#Before moving any further, recall that we used  the condition below to split the training and test set. Let's proceed with this in mind
# ***** cond = ((df.saleYear<=2010) | ((df.saleYear==2011) & (df.saleMonth<=10))) ***** 

#From what we see above , saleElapsed is used as the main identifier to see if the model is part of the training or validaiton set.
#saleElapsed tells us how long it has been since the sale took place , this is a very good pointer as to whether the machine was
#sold before or after October 2011. salesID also seems to have some form of time element attached
#to it. As for MachineID we could say that some models of bulldozers(identifier being their MachineID ) were only made after a certain
#year, since we used the saleYear to split the test and validation tests , a tractor can only be sold during or after 
#the year of its production .For example if you gave an Apple employee your phone's Serial Number 
#he/she will be able to tell you what model of Iphone you're carrying and hence the year that it was 
# released and thus he will be at least able to predict with decent accuracy, the years you bought the phone (you can only buy it
#on the year of release or the years after)..

m = rf(xs_final, y)
print('orig', m_rmse(m, valid_xs_final, valid_y))

for c in ('SalesID', 'saleElapsed','MachineID'):
    m = rf(xs_final.drop(c,axis=1), y)
    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))

time_vars = ['SalesID','MachineID']
#We remove SalesID and MachineID since it gives us better accuracies
xs_final_time = xs_final.drop(time_vars, axis=1)
valid_xs_time = valid_xs_final.drop(time_vars, axis=1)

m = rf(xs_final_time, y)
m_rmse(m, valid_xs_time, valid_y)

#Another thing that can help is avoiding old data , old data ususally provides relationships that may not be true anymore...

xs['saleYear'].hist();

filt_year = xs['saleYear']>2004#let's only keep data after 2004
xs_filt_year= xs_final_time[filt_year]
y_filt_year = y[filt_year]

m = rf(xs_filt_year, y_filt_year)
m_rmse(m, xs_filt_year, y_filt_year), m_rmse(m, valid_xs_time, valid_y)

